#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å°çº¢ä¹¦çˆ¬è™«åˆ°å¤§ä¼—ç‚¹è¯„æ–‡æ¡ˆç”Ÿæˆè„šæœ¬
åŠŸèƒ½ï¼š
1. è°ƒç”¨å°çº¢ä¹¦çˆ¬è™«æœç´¢æŒ‡å®šå…³é”®è¯
2. æŒ‰ç‚¹èµæ•°æ’åºçˆ¬å–100ä¸ªå¸–å­å¹¶å­˜å‚¨åˆ°MySQL
3. ä½¿ç”¨å¤§æ¨¡å‹ç­›é€‰æœ€ä½³9ä¸ªå›¾ç‰‡
4. ç”Ÿæˆé€‚åˆå¤§ä¼—ç‚¹è¯„çš„ç¾é£Ÿæµ‹è¯„æ–‡æ¡ˆ
"""

import asyncio
import json
import os
import sys
import logging
from typing import List, Dict, Optional
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config
import db
from media_platform.xhs import XiaoHongShuCrawler
from store.xhs.xhs_store_sql import query_content_by_content_id
from db import AsyncMysqlDB
from var import media_crawler_db_var

# AIæ¨¡å‹ç›¸å…³å¯¼å…¥
try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    print("Warning: openai package not installed. AI features will use template-based generation.")

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    print("Warning: requests package not installed. Local AI model support disabled.")


@dataclass
class RestaurantPost:
    """é¤å…å¸–å­æ•°æ®ç±»"""
    note_id: str
    title: str
    desc: str
    liked_count: int
    comment_count: int
    collected_count: int
    image_list: List[str]
    note_url: str
    nickname: str


class AIModelManager:
    """AIæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.openai_base_url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
        self.local_model_url = os.getenv('LOCAL_MODEL_URL', 'http://localhost:11434/api/generate')
        
        if self.openai_api_key and HAS_OPENAI:
            openai.api_key = self.openai_api_key
            openai.base_url = self.openai_base_url
    
    async def analyze_images_content(self, image_candidates: List[Dict], keyword: str, num_images: int = 9) -> List[str]:
        """ä½¿ç”¨AIåˆ†æå›¾ç‰‡å†…å®¹å¹¶ç­›é€‰æœ€ä½³å›¾ç‰‡"""
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""
        ä½œä¸ºç¾é£Ÿå›¾ç‰‡åˆ†æä¸“å®¶ï¼Œè¯·ä»ä»¥ä¸‹å›¾ç‰‡å€™é€‰åˆ—è¡¨ä¸­ç­›é€‰å‡ºæœ€é€‚åˆ{keyword}ä¸»é¢˜çš„{num_images}å¼ å›¾ç‰‡ã€‚

        ç­›é€‰æ ‡å‡†ï¼š
        1. å›¾ç‰‡å†…å®¹ä¸{keyword}é«˜åº¦ç›¸å…³
        2. å›¾ç‰‡è´¨é‡é«˜ï¼Œæ„å›¾ç¾è§‚
        3. èƒ½ä½“ç°é£Ÿç‰©çš„è‰²é¦™å‘³
        4. é€‚åˆåœ¨å¤§ä¼—ç‚¹è¯„ç­‰ç¾é£Ÿå¹³å°å±•ç¤º
        5. å›¾ç‰‡æ¥æºå¸–å­çš„ç‚¹èµæ•°è¾ƒé«˜

        å€™é€‰å›¾ç‰‡ä¿¡æ¯ï¼š
        {self._format_image_candidates_for_ai(image_candidates[:30])}

        è¯·ç›´æ¥è¿”å›ç­›é€‰å‡ºçš„å›¾ç‰‡URLåˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªURLï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šã€‚
        """
        
        try:
            if self.openai_api_key and HAS_OPENAI:
                response = await self._call_openai(prompt)
            else:
                response = await self._call_local_model(prompt)
            
            # è§£æAIè¿”å›çš„URLåˆ—è¡¨
            urls = [line.strip() for line in response.split('\n') if line.strip() and line.strip().startswith('http')]
            return urls[:num_images]
            
        except Exception as e:
            logging.warning(f"AIå›¾ç‰‡ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•: {e}")
            return await self._heuristic_image_selection(image_candidates, num_images)
    
    async def generate_dianping_content(self, reference_posts: List[Dict], keyword: str, selected_images: List[str]) -> str:
        """ä½¿ç”¨AIç”Ÿæˆå¤§ä¼—ç‚¹è¯„é£æ ¼çš„æ–‡æ¡ˆ"""
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä½ä¼˜ç§€çš„ç¾é£Ÿæ¢åº—è¾¾äººï¼Œè¯·å¸®æˆ‘å†™ä¸€ç¯‡é’ˆå¯¹{keyword}é¤å…çš„ç¾é£Ÿæµ‹è¯„ã€‚

        è¦æ±‚ï¼š
        1. è¯·ä»å£å‘³å–ç›¸ã€æœåŠ¡ã€ç¯å¢ƒã€ä»·æ ¼çš„æ–¹é¢å»åšç‚¹è¯„
        2. å¯ä»¥ç”¨ä¸€äº›è¡¨æƒ…ç¬¦å·ï¼Œä½†æ˜¯ä¸è¦ç”¨-å’Œ**ç­‰æ ¼å¼ç¬¦å·
        3. è¦æ±‚200-300å­—å·¦å³
        4. å†™å‡ºä¸€ä¸ªç‰¹åˆ«æœ‰ç”»é¢æ„Ÿçš„æ ‡é¢˜
        5. è¯­è¨€é£æ ¼è¦ç¬¦åˆå¤§ä¼—ç‚¹è¯„ç”¨æˆ·çš„ä¹ æƒ¯ï¼ŒçœŸå®è‡ªç„¶

        å‚è€ƒä¿¡æ¯ï¼ˆæ¥è‡ªå°çº¢ä¹¦é«˜ç‚¹èµå¸–å­ï¼‰ï¼š
        {self._format_posts_for_ai(reference_posts[:10])}

        å·²ç­›é€‰çš„å›¾ç‰‡æ•°é‡ï¼š{len(selected_images)}å¼ ç²¾ç¾å›¾ç‰‡

        è¯·ç”Ÿæˆä¸€ç¯‡å®Œæ•´çš„å¤§ä¼—ç‚¹è¯„é£æ ¼æµ‹è¯„ï¼ŒåŒ…å«æ ‡é¢˜å’Œæ­£æ–‡ï¼š
        """
        
        try:
            if self.openai_api_key and HAS_OPENAI:
                response = await self._call_openai(prompt)
            else:
                response = await self._call_local_model(prompt)
            
            return response.strip()
            
        except Exception as e:
            logging.warning(f"AIæ–‡æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿æ–¹æ³•: {e}")
            return await self._generate_template_content(reference_posts, keyword)
    
    async def _call_openai(self, prompt: str) -> str:
        """è°ƒç”¨OpenAI API"""
        try:
            response = await openai.ChatCompletion.acreate(
                model=os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo'),
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¾é£Ÿå†…å®¹åˆ›ä½œè€…å’Œå›¾ç‰‡åˆ†æå¸ˆã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
    
    async def _call_local_model(self, prompt: str) -> str:
        """è°ƒç”¨æœ¬åœ°æ¨¡å‹APIï¼ˆå¦‚Ollamaï¼‰"""
        if not HAS_REQUESTS:
            raise Exception("requestsåº“æœªå®‰è£…ï¼Œæ— æ³•è°ƒç”¨æœ¬åœ°æ¨¡å‹")
        
        try:
            payload = {
                "model": os.getenv('LOCAL_MODEL_NAME', 'llama2'),
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(self.local_model_url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result.get('response', '')
            
        except Exception as e:
            raise Exception(f"æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
    
    def _format_image_candidates_for_ai(self, candidates: List[Dict]) -> str:
        """æ ¼å¼åŒ–å›¾ç‰‡å€™é€‰ä¿¡æ¯ä¾›AIåˆ†æ"""
        formatted = []
        for i, candidate in enumerate(candidates, 1):
            formatted.append(f"{i}. URL: {candidate['url']}")
            formatted.append(f"   å¸–å­æ ‡é¢˜: {candidate['post_title']}")
            formatted.append(f"   ç‚¹èµæ•°: {candidate['liked_count']}")
            formatted.append(f"   æè¿°ç‰‡æ®µ: {candidate['post_desc'][:100]}...")
            formatted.append("")
        return "\n".join(formatted)
    
    def _format_posts_for_ai(self, posts: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¸–å­ä¿¡æ¯ä¾›AIå‚è€ƒ"""
        formatted = []
        for i, post in enumerate(posts, 1):
            formatted.append(f"{i}. æ ‡é¢˜: {post['title']}")
            formatted.append(f"   æè¿°: {post['desc'][:200]}...")
            formatted.append(f"   ç‚¹èµæ•°: {post['liked_count']}")
            formatted.append("")
        return "\n".join(formatted)
    
    async def _heuristic_image_selection(self, candidates: List[Dict], num_images: int) -> List[str]:
        """å¯å‘å¼å›¾ç‰‡ç­›é€‰ï¼ˆAIè°ƒç”¨å¤±è´¥æ—¶çš„åå¤‡æ–¹æ¡ˆï¼‰"""
        candidates.sort(key=lambda x: x['liked_count'], reverse=True)
        
        selected = []
        seen_urls = set()
        
        for candidate in candidates:
            if len(selected) >= num_images:
                break
                
            url = candidate['url']
            if url not in seen_urls and url.strip():
                selected.append(url)
                seen_urls.add(url)
        
        return selected[:num_images]
    
    async def _generate_template_content(self, reference_posts: List[Dict], keyword: str) -> str:
        """åŸºäºæ¨¡æ¿ç”Ÿæˆæ–‡æ¡ˆï¼ˆAIè°ƒç”¨å¤±è´¥æ—¶çš„åå¤‡æ–¹æ¡ˆï¼‰"""
        # ä»å‚è€ƒå†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯
        keywords_found = []
        for post in reference_posts:
            desc = post.get('desc', '')
            title = post.get('title', '')
            
            # æå–å£å‘³ã€ç¯å¢ƒã€æœåŠ¡ç›¸å…³çš„å…³é”®è¯
            text = f"{title} {desc}".lower()
            
            if any(word in text for word in ['å¥½åƒ', 'ç¾å‘³', 'é¦™', 'å«©', 'é²œ']):
                keywords_found.append('å£å‘³ä½³')
            if any(word in text for word in ['ç¯å¢ƒ', 'è£…ä¿®', 'æ°›å›´', 'åº—é¢']):
                keywords_found.append('ç¯å¢ƒå¥½')
            if any(word in text for word in ['æœåŠ¡', 'æ€åº¦', 'çƒ­æƒ…']):
                keywords_found.append('æœåŠ¡å¥½')
            if any(word in text for word in ['åˆ’ç®—', 'ä¾¿å®œ', 'å®æƒ ', 'æ€§ä»·æ¯”']):
                keywords_found.append('æ€§ä»·æ¯”é«˜')
        
        # ç”Ÿæˆæ ‡é¢˜
        title = f"æ¢åº—{keyword} | è¿™å®¶åº—çœŸçš„ç»äº†ï¼ğŸ“¸âœ¨"
        
        # ç”Ÿæˆæ­£æ–‡
        content_parts = [
            f"ğŸ– ä»Šå¤©æ¥æ¢åº—ä¼ è¯´ä¸­çš„{keyword}ï¼ŒçœŸçš„æ˜¯è¢«æƒŠè‰³åˆ°äº†ï¼",
            "",
            "ã€å£å‘³å–ç›¸ã€‘â­â­â­â­â­",
            "çƒ¤è‚‰çš„ç«å€™æŒæ¡å¾—åˆšåˆšå¥½ï¼Œå¤–ç„¦å†…å«©ï¼Œæ¯ä¸€å£éƒ½èƒ½æ„Ÿå—åˆ°è‚‰æ±çš„é²œç¾ğŸ’§ æ‘†ç›˜ä¹Ÿå¾ˆç”¨å¿ƒï¼Œé¢œå€¼å’Œå‘³é“éƒ½åœ¨çº¿ğŸ‘",
            "",
            "ã€æœåŠ¡ä½“éªŒã€‘â­â­â­â­",
            "æœåŠ¡å‘˜å°å§å§è¶…çº§çƒ­æƒ…ï¼Œä¼šä¸»åŠ¨å¸®å¿™çƒ¤è‚‰ï¼Œè¿˜ä¼šæ¨èå¥½åƒçš„æ­é…ğŸ¥° æ•´ä¸ªç”¨é¤è¿‡ç¨‹å¾ˆæ„‰å¿«",
            "",
            "ã€ç¯å¢ƒæ°›å›´ã€‘â­â­â­â­",
            "åº—å†…è£…ä¿®å¾ˆæœ‰ç‰¹è‰²ï¼Œç¯å…‰æ°›å›´è¥é€ å¾—å¾ˆæ£’âœ¨ é€‚åˆå’Œæœ‹å‹èšé¤ï¼Œæ‹ç…§ä¹Ÿå¾ˆå¥½çœ‹ğŸ“·",
            "",
            "ã€ä»·æ ¼æ°´å¹³ã€‘â­â­â­â­",
            "äººå‡æ¶ˆè´¹åˆç†ï¼Œåˆ†é‡è¶³å¤Ÿï¼Œæ€§ä»·æ¯”è¿˜æ˜¯å¾ˆä¸é”™çš„ğŸ’° å­¦ç”Ÿå…šä¹Ÿå¯ä»¥æ‰¿å—",
            "",
            f"æ€»çš„æ¥è¯´ï¼Œ{keyword}çœŸçš„å€¼å¾—ä¸€è¯•ï¼å·²ç»é¢„çº¦ä¸‹æ¬¡å¸¦å®¶äººæ¥äº†ğŸ‰",
            "",
            "#ç¾é£Ÿæ¢åº— #çƒ¤è‚‰æ¨è #åŒ—äº¬ç¾é£Ÿ"
        ]
        
        return f"{title}\n\n" + "\n".join(content_parts)


class XhsToDianpingGenerator:
    """å°çº¢ä¹¦åˆ°å¤§ä¼—ç‚¹è¯„æ–‡æ¡ˆç”Ÿæˆå™¨"""
    
    def __init__(self, keyword: str):
        self.keyword = keyword
        self.posts: List[RestaurantPost] = []
        self.selected_images: List[str] = []
        self.generated_content: str = ""
        self.ai_manager = AIModelManager()
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def crawl_xhs_posts(self, max_posts: int = 100) -> None:
        """
        çˆ¬å–å°çº¢ä¹¦å¸–å­
        
        Args:
            max_posts: æœ€å¤§çˆ¬å–å¸–å­æ•°é‡
        """
        self.logger.info(f"å¼€å§‹çˆ¬å–å°çº¢ä¹¦å…³é”®è¯: {self.keyword}")
        
        # ä¸´æ—¶ä¿®æ”¹é…ç½®
        original_keywords = config.KEYWORDS
        original_sort_type = config.SORT_TYPE
        original_max_notes = config.CRAWLER_MAX_NOTES_COUNT
        original_save_option = config.SAVE_DATA_OPTION
        
        try:
            # è®¾ç½®çˆ¬è™«é…ç½®
            config.KEYWORDS = self.keyword
            config.SORT_TYPE = "popularity_descending"  # æŒ‰ç‚¹èµæ•°é™åº
            config.CRAWLER_MAX_NOTES_COUNT = max_posts
            config.SAVE_DATA_OPTION = "db"  # ä¿å­˜åˆ°æ•°æ®åº“
            config.CRAWLER_TYPE = "search"
            config.ENABLE_GET_IMAGES = True  # å¯ç”¨å›¾ç‰‡çˆ¬å–
            config.ENABLE_GET_COMMENTS = False  # ä¸éœ€è¦è¯„è®ºï¼Œæé«˜æ•ˆç‡
            
            # åˆå§‹åŒ–æ•°æ®åº“
            await db.init_db()
            
            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            crawler = XiaoHongShuCrawler()
            
            # å¼€å§‹çˆ¬å–
            await crawler.start()
            
            self.logger.info(f"å®Œæˆçˆ¬å–ï¼Œå¼€å§‹ä»æ•°æ®åº“è·å–æ•°æ®")
            
        except Exception as e:
            self.logger.error(f"çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}")
            raise
        finally:
            # æ¢å¤åŸå§‹é…ç½®
            config.KEYWORDS = original_keywords
            config.SORT_TYPE = original_sort_type
            config.CRAWLER_MAX_NOTES_COUNT = original_max_notes
            config.SAVE_DATA_OPTION = original_save_option
    
    async def get_top_posts_from_db(self, limit: int = 100) -> List[RestaurantPost]:
        """
        ä»æ•°æ®åº“è·å–ç‚¹èµæœ€é«˜çš„å¸–å­
        
        Args:
            limit: è·å–å¸–å­æ•°é‡é™åˆ¶
            
        Returns:
            å¸–å­åˆ—è¡¨
        """
        self.logger.info("ä»æ•°æ®åº“è·å–ç‚¹èµæœ€é«˜çš„å¸–å­")
        
        async_db_conn: AsyncMysqlDB = media_crawler_db_var.get()
        
        # æŸ¥è¯¢ç‚¹èµæ•°æœ€é«˜çš„å¸–å­ï¼ŒæŒ‰ç‚¹èµæ•°é™åºæ’åˆ—
        sql = f"""
        SELECT note_id, title, `desc`, liked_count, comment_count, collected_count, 
               image_list, note_url, nickname
        FROM xhs_note 
        WHERE source_keyword = '{self.keyword}'
        AND liked_count IS NOT NULL 
        AND liked_count != ''
        AND image_list IS NOT NULL
        AND image_list != ''
        ORDER BY CAST(liked_count AS UNSIGNED) DESC
        LIMIT {limit}
        """
        
        try:
            rows = await async_db_conn.query(sql)
            posts = []
            
            for row in rows:
                # å¤„ç†å›¾ç‰‡åˆ—è¡¨
                image_list = []
                if row.get('image_list'):
                    image_list = [img.strip() for img in row['image_list'].split(',') if img.strip()]
                
                # è½¬æ¢ç‚¹èµæ•°ä¸ºæ•´æ•°
                liked_count = 0
                try:
                    liked_count = int(row.get('liked_count', 0) or 0)
                except (ValueError, TypeError):
                    liked_count = 0
                
                post = RestaurantPost(
                    note_id=row['note_id'],
                    title=row.get('title', ''),
                    desc=row.get('desc', ''),
                    liked_count=liked_count,
                    comment_count=int(row.get('comment_count', 0) or 0),
                    collected_count=int(row.get('collected_count', 0) or 0),
                    image_list=image_list,
                    note_url=row.get('note_url', ''),
                    nickname=row.get('nickname', '')
                )
                posts.append(post)
            
            self.logger.info(f"è·å–åˆ° {len(posts)} ä¸ªå¸–å­")
            self.posts = posts
            return posts
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“æŸ¥è¯¢å‡ºé”™: {e}")
            raise
    
    async def select_best_images_with_ai(self, num_images: int = 9) -> List[str]:
        """
        ä½¿ç”¨AIç­›é€‰æœ€ä½³å›¾ç‰‡
        
        Args:
            num_images: éœ€è¦ç­›é€‰çš„å›¾ç‰‡æ•°é‡
            
        Returns:
            ç­›é€‰å‡ºçš„å›¾ç‰‡URLåˆ—è¡¨
        """
        self.logger.info(f"ä½¿ç”¨AIç­›é€‰æœ€ä½³ {num_images} å¼ å›¾ç‰‡")
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡URLå’Œå¯¹åº”çš„å¸–å­ä¿¡æ¯
        image_candidates = []
        for post in self.posts[:50]:  # åªè€ƒè™‘å‰50ä¸ªé«˜ç‚¹èµå¸–å­
            for img_url in post.image_list:
                image_candidates.append({
                    'url': img_url,
                    'post_title': post.title,
                    'post_desc': post.desc,
                    'liked_count': post.liked_count,
                    'post_url': post.note_url
                })
        
        # ä½¿ç”¨AIæ¨¡å‹ç­›é€‰å›¾ç‰‡
        selected_images = await self.ai_manager.analyze_images_content(
            image_candidates, self.keyword, num_images
        )
        
        self.selected_images = selected_images
        self.logger.info(f"ç­›é€‰å®Œæˆï¼Œé€‰å‡º {len(selected_images)} å¼ å›¾ç‰‡")
        
        return selected_images
    
    async def generate_dianping_content_with_ai(self) -> str:
        """
        ä½¿ç”¨AIç”Ÿæˆå¤§ä¼—ç‚¹è¯„é£æ ¼çš„æ–‡æ¡ˆ
        
        Returns:
            ç”Ÿæˆçš„æ–‡æ¡ˆå†…å®¹
        """
        self.logger.info("ç”Ÿæˆå¤§ä¼—ç‚¹è¯„é£æ ¼æ–‡æ¡ˆ")
        
        # æ”¶é›†å¸–å­å†…å®¹ç”¨äºå‚è€ƒ
        reference_content = []
        for post in self.posts[:20]:  # å–å‰20ä¸ªé«˜è´¨é‡å¸–å­ä½œä¸ºå‚è€ƒ
            reference_content.append({
                'title': post.title,
                'desc': post.desc,
                'liked_count': post.liked_count
            })
        
        # ä½¿ç”¨AIç”Ÿæˆæ–‡æ¡ˆ
        content = await self.ai_manager.generate_dianping_content(
            reference_content, self.keyword, self.selected_images
        )
        
        self.generated_content = content
        self.logger.info("æ–‡æ¡ˆç”Ÿæˆå®Œæˆ")
        
        return content
    
    async def run(self) -> Dict[str, any]:
        """
        è¿è¡Œå®Œæ•´æµç¨‹
        
        Returns:
            åŒ…å«ç»“æœçš„å­—å…¸
        """
        try:
            # 1. çˆ¬å–å°çº¢ä¹¦å¸–å­
            await self.crawl_xhs_posts(100)
            
            # 2. ä»æ•°æ®åº“è·å–ç‚¹èµæœ€é«˜çš„å¸–å­
            posts = await self.get_top_posts_from_db(100)
            
            if not posts:
                raise ValueError("æœªè·å–åˆ°ä»»ä½•å¸–å­æ•°æ®")
            
            # 3. ç­›é€‰æœ€ä½³å›¾ç‰‡
            images = await self.select_best_images_with_ai(9)
            
            # 4. ç”Ÿæˆæ–‡æ¡ˆ
            content = await self.generate_dianping_content_with_ai()
            
            result = {
                'keyword': self.keyword,
                'total_posts': len(posts),
                'selected_images': images,
                'content': content,
                'top_posts': [
                    {
                        'title': post.title,
                        'liked_count': post.liked_count,
                        'note_url': post.note_url
                    } for post in posts[:10]
                ]
            }
            
            self.logger.info("ä»»åŠ¡å®Œæˆï¼")
            return result
            
        except Exception as e:
            self.logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            raise
        finally:
            # å…³é—­æ•°æ®åº“è¿æ¥
            await db.close()


async def main():
    """ä¸»å‡½æ•°"""
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–ç¯å¢ƒå˜é‡è®¾ç½®å…³é”®è¯
    keyword = os.getenv('KEYWORD', 'åŒ—äº¬æœ¬å’ç¾å¼çƒ¤è‚‰')
    
    if len(sys.argv) > 1:
        keyword = sys.argv[1]
    
    generator = XhsToDianpingGenerator(keyword)
    
    try:
        result = await generator.run()
        
        # è¾“å‡ºç»“æœ
        print("=" * 50)
        print("å°çº¢ä¹¦åˆ°å¤§ä¼—ç‚¹è¯„æ–‡æ¡ˆç”Ÿæˆç»“æœ")
        print("=" * 50)
        print(f"å…³é”®è¯: {result['keyword']}")
        print(f"è·å–å¸–å­æ•°: {result['total_posts']}")
        print(f"ç­›é€‰å›¾ç‰‡æ•°: {len(result['selected_images'])}")
        print("\n" + "=" * 30)
        print("ç”Ÿæˆçš„æ–‡æ¡ˆ:")
        print("=" * 30)
        print(result['content'])
        
        print("\n" + "=" * 30)
        print("ç­›é€‰çš„å›¾ç‰‡:")
        print("=" * 30)
        for i, img_url in enumerate(result['selected_images'], 1):
            print(f"{i}. {img_url}")
        
        print("\n" + "=" * 30)
        print("å‚è€ƒçš„çƒ­é—¨å¸–å­:")
        print("=" * 30)
        for i, post in enumerate(result['top_posts'], 1):
            print(f"{i}. {post['title']} (ç‚¹èµ: {post['liked_count']})")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = f"output_{keyword.replace(' ', '_')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())


